{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changelog\n",
    "- 3 Oct\n",
    "    - Creation\n",
    "    - Add `Tables`, `DataValueInterfaces` and `DataAPI` because `PGFPlots` wasn't getting precompiled\n",
    "    - Wrote functions for regressogram,local_averaging and Gaussian kernel regression\n",
    "    - Works fine on Glass. Regressogram not working on motorcycle dataset\n",
    "- 7 Oct\n",
    "    - Motorcyle regressogram works; was simply a matter of name confusion between glass and motorcyle\n",
    "- 8 Oct\n",
    "    - All the other questions\n",
    "    - Saving pgfplots\n",
    "- 16 Oct\n",
    "    - Hw2 local linear regression implementation\n",
    "- 22 Oct\n",
    "    - Hw2 part b LOOCV implementation and optimal bandwidth for motorcycle dataset\n",
    "- 16 Nov\n",
    "    - Homework 4: 2 layer wide neural network\n",
    "- 18 Nov\n",
    "    - Continuing work on hw4. Actually deploying the functions to do some compuation now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "using DelimitedFiles # To use readdlm\n",
    "using PGFPlots\n",
    "using StatsBase # To use mean\n",
    "using Distributions # To call Normal distribution\n",
    "using Random # To seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     10,
     32
    ]
   },
   "outputs": [],
   "source": [
    "# function: make even bins\n",
    "\"\"\"\n",
    "    function make_bins(num_bins,lo,hi)\n",
    "- Make `num_bins` even bins from `lo` to `hi`\n",
    "\n",
    "# Example\n",
    "```julia\n",
    "bin_lefts = make_bins(10,0.29,3.50)\n",
    "```\n",
    "\"\"\"\n",
    "function make_bins(num_bins,lo,hi;verbosity=false)\n",
    "    bin_array = fill(0.,num_bins)\n",
    "    if verbosity print(\"lo=$lo,hi=$hi,num_bins=$num_bins\\n\") end\n",
    "    width = (hi-lo)/num_bins\n",
    "    if verbosity print(\"width=$width\\n\") end\n",
    "    for i in 1:num_bins\n",
    "        bin_array[i] = lo+width*(i-1)\n",
    "    end\n",
    "    return bin_array\n",
    "end\n",
    "\n",
    "# function: Create dict with bins and corresponding average outputs\n",
    "\"\"\"\n",
    "    function binned_y_average(binned_y_train)\n",
    "- Given a dict with bin number as keys and array of output values as value\n",
    "- Return dict with same bin number keys but just the corresponding mean value\n",
    "\n",
    "# Example\n",
    "```julia\n",
    "binavg_y_train = binned_y_average(binned_y_train)\n",
    "```\n",
    "\"\"\"\n",
    "function binned_y_average(binned_y_train)\n",
    "    binavg_y_train = Dict{Int64,Float64}()\n",
    "    for (k,v) in binned_y_train\n",
    "        binavg_y_train[k] = mean(v)\n",
    "    end\n",
    "    return binavg_y_train\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function: Regressogram\n",
    "\"\"\"\n",
    "    function regressogram(num_bins,lo,hi,num_test,x_train,y_train)\n",
    "- Divide input (covariate) range into `num_bins` from `lo` to `hi`\n",
    "- Run regressogram on `num_test` evenly spaced input points in the range `lo` to `hi`\n",
    "- Training is done using `x_train` and `y_train`\n",
    "\n",
    "# How it works\n",
    "- Collect training data input values into uniform bins\n",
    "- Caculate corresponding average y value\n",
    "- Test input is placed into bin and associated output is average computed above\n",
    "\n",
    "# Examples\n",
    "```julia\n",
    "g = readdlm(\"glass.dat\");y_train = Float64.(a[2:end,2]);x_train=Float64.(a[2:end,5]);\n",
    "p_regressogram = regressogram(10,0.29,3.50,200,x_train,y_train)\n",
    "```\n",
    "\"\"\"\n",
    "function regressogram(num_bins,lo,hi,num_test,x_train,y_train)\n",
    "    bin_lefts = make_bins(num_bins,lo,hi);\n",
    "\n",
    "    binned_y_train = Dict{Int64,Array{Float64,1}}()\n",
    "    binned_x_train = Dict{Int64,Array{Float64,1}}()\n",
    "    # initialize the dictionary\n",
    "    for i in 1:num_bins\n",
    "        binned_y_train[i] = []\n",
    "        binned_x_train[i] = []\n",
    "    end\n",
    "    # Go over the training set and push the y values into bins to take mean later\n",
    "    for (j,x_train_sample) in enumerate(x_train)\n",
    "        push!(binned_y_train[argmin(abs.(x_train_sample .- bin_lefts))],y_train[j])\n",
    "        push!(binned_x_train[argmin(abs.(x_train_sample .- bin_lefts))],x_train_sample)\n",
    "    end\n",
    "    binavg_y_train = binned_y_average(binned_y_train)\n",
    "    \n",
    "    x_test = make_bins(num_test,lo,hi);\n",
    "    y_test = fill(0.,length(x_test),)\n",
    "    for (i,x_test_sample) in enumerate(x_test)\n",
    "        y_test[i] = binavg_y_train[argmin(abs.(x_test_sample .- bin_lefts))]\n",
    "    end\n",
    "    plot_regressogram = PGFPlots.Plots.Scatter(x_test,y_test,legendentry=\"Regressogram\");\n",
    "    return plot_regressogram\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function: local_averaging\n",
    "\"\"\"\n",
    "    function local_averaging(bandwidth,lo,hi,num_test,x_train,y_train)\n",
    "`lo` and `hi` are used to create `num_test` test points. They don't directly work in the algo\n",
    "\n",
    "# How it works\n",
    "- For every test input, consider the training inputs within `bandwidth` from the test input\n",
    "- The associated test output is the average training outputs of these points within the bandwidth\n",
    "\n",
    "# Examples\n",
    "```julia\n",
    "p_lavg = local_averaging(0.321,0.29,3.50,200,x_train,y_train)\n",
    "```\n",
    "\"\"\"\n",
    "function local_averaging(bandwidth,lo,hi,num_test,x_train,y_train)\n",
    "    x_test = make_bins(num_test,lo,hi)\n",
    "    y_test_localavg = fill(0.,length(x_test),)\n",
    "    for (i,x_test_sample) in enumerate(x_test)\n",
    "        x_neighbors = []\n",
    "        y_neighbors = []\n",
    "        for (kk,x_train_sample) in enumerate(x_train)\n",
    "            if abs(x_train_sample-x_test_sample) <= bandwidth\n",
    "                push!(x_neighbors,x_train_sample)\n",
    "                push!(y_neighbors,y_train[kk])\n",
    "            end\n",
    "        end\n",
    "        y_test_localavg[i] = mean(y_neighbors)\n",
    "    end\n",
    "    plot_localavg = PGFPlots.Plots.Scatter(x_test,y_test_localavg,legendentry=\"Local Averaging\");\n",
    "    return plot_localavg\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function: Gaussian Kernel\n",
    "\"\"\"\n",
    "    function gaussian_kernel(bandwidth,lo,hi,num_test,x_train,y_train)\n",
    "\n",
    "# How it works\n",
    "- For every test input, assign weights to all the training points\n",
    "- These weights determine how much every training point contributes to the test output\n",
    "- The weights is calculated based on the Gaussian pdf form of distance calculator\n",
    "\n",
    "# Examples\n",
    "```julia\n",
    "p_gauss = gaussian_kernel(0.321,0.29,3.50,200,x_train,y_train)\n",
    "```\n",
    "\"\"\"\n",
    "function gaussian_kernel(bandwidth,lo,hi,num_test,x_train,y_train)\n",
    "    x_test = make_bins(num_test,lo,hi)\n",
    "    y_test_gaussian = fill(0.,length(x_test),)\n",
    "    for (i,x_test_sample) in enumerate(x_test)\n",
    "        weights = fill(0.,length(x_train),)\n",
    "\n",
    "        for (kk,x_train_sample) in enumerate(x_train)\n",
    "            weights[kk] = (1/(sqrt(2*3.14)))*exp((-1/2)*((x_train_sample-x_test_sample)/bandwidth)^2)\n",
    "        end\n",
    "        y_test_gaussian[i] = (sum(weights.*y_train))/sum(weights)\n",
    "    end\n",
    "    plot_gaussian = PGFPlots.Plots.Scatter(x_test,y_test_gaussian,legendentry=\"Gaussian Kernel\");\n",
    "    return plot_gaussian\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glass dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Compare various regressions for glass dataset\n",
    "g = readdlm(\"glass.dat\")\n",
    "x_train_g=Float64.(g[2:end,5])\n",
    "y_train_g = Float64.(g[2:end,2])\n",
    "plot_trainingdata = PGFPlots.Plots.Scatter(x_train_g,y_train_g,legendentry=\"Train\");\n",
    "num_bins_g = 10 \n",
    "lo_g=0.29\n",
    "hi_g=3.50\n",
    "num_test_g=200\n",
    "bandwidth_g=0.321;\n",
    "p_regressogram = regressogram(num_bins_g,lo_g,hi_g,num_test_g,x_train_g,y_train_g)\n",
    "p_lavg = local_averaging(bandwidth_g,lo_g,hi_g,num_test_g,x_train_g,y_train_g)\n",
    "p_gauss = gaussian_kernel(bandwidth_g,lo_g,hi_g,num_test_g,x_train_g,y_train_g)\n",
    "fig = PGFPlots.Axis([plot_trainingdata,p_regressogram,p_lavg,p_gauss],title=\"Glass Dataset\",\n",
    "    xlabel=\"Aluminium Content\",ylabel=\"Refractive Index\",legendPos=\"outer north east\")\n",
    "display(fig)\n",
    "PGFPlots.save(\"figs/glass.tikz\",fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### motorcycle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare approaches on motorcyle dataset\n",
    "m = readdlm(\"motor.dat\"); y_train_motor = Float64.(m[2:end,2]); x_train_motor = Float64.(m[2:end,1]);\n",
    "plot_trainingdata = PGFPlots.Plots.Scatter(x_train_motor,y_train_motor,legendentry=\"Train\");\n",
    "num_bins = 20; lo_m=2.4;hi_m=57.6;num_test=200;bandwidth=2.76;\n",
    "p_reg = regressogram(num_bins,lo_m,hi_m,num_test,x_train_motor,y_train_motor)\n",
    "p_lavg = local_averaging(bandwidth,lo_m,hi_m,num_test,x_train_motor,y_train_motor)\n",
    "p_gauss = gaussian_kernel(bandwidth,lo_m,hi_m,num_test,x_train_motor,y_train_motor)\n",
    "fig_m = PGFPlots.Axis([plot_trainingdata,p_reg,p_lavg,p_gauss],title=\"Motorcycle Dataset\",\n",
    "    xlabel=\"Time\",ylabel=\"Acceleration\",legendPos=\"outer north east\")\n",
    "display(fig_m)\n",
    "PGFPlots.save(\"figs/motor.tikz\",fig_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part b) Bias variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "- $r(x) = x, \\sigma= 1, n = 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's generate the synthetic data\n",
    "Random.seed!(1)\n",
    "x = collect(0.01:0.01:1);y = x + rand(Normal(0,1),100);\n",
    "r_x = x;\n",
    "p_noisy = PGFPlots.Plots.Scatter(x,y,legendentry=\"noisy data\")\n",
    "p_ground = PGFPlots.Plots.Scatter(x,r_x,legendentry=\"ground truth\")\n",
    "PGFPlots.Axis([p_noisy,p_ground],xlabel=\"x\",ylabel=\"y\",legendPos=\"outer north east\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train our regressogram and local averaging and plot their output\n",
    "p_regressogram = regressogram(10,0,1,200,x,r_x)\n",
    "p_lavg = local_averaging(0.1,0,1,200,x,r_x)\n",
    "fig_b1 = PGFPlots.Axis([p_ground,p_regressogram,p_lavg],legendPos=\"outer north east\")\n",
    "display(fig_b1)\n",
    "PGFPlots.save(\"figs/b1.tikz\",fig_b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "- $r(x) = sin(2\\pi x),\\sigma=1,n=99$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function: Gaussian Kernel for MSE\n",
    "\"\"\"\n",
    "    function gaussian_kernel_for_MSE(bandwidth,lo,hi,num_test,x_train,y_train,sigma)\n",
    "Returns the analytical mean squared error\n",
    "\n",
    "# How it works\n",
    "- For every test input, assign weights to all the training points\n",
    "- These weights determine how much every training point contributes to the test output\n",
    "- The weights is calculated based on the Gaussian pdf form of distance calculator\n",
    "- sigma is the std. dev of the corrupting noise on the training data\n",
    "\n",
    "# Examples\n",
    "```julia\n",
    "gaussian_kernel_for_MSE(0.1,1/99,1.,99,x,r_x,1.)\n",
    "```\n",
    "\"\"\"\n",
    "function gaussian_kernel_for_MSE(bandwidth,lo,hi,num_test,x_train,y_train,sigma)\n",
    "    x_test = make_bins(num_test,lo,hi)\n",
    "    y_test_gaussian = fill(0.,length(x_test),)\n",
    "    var = fill(0.,length(x_test),)\n",
    "    for (i,x_test_sample) in enumerate(x_test)\n",
    "        weights = fill(0.,length(x_train),)\n",
    "\n",
    "        for (kk,x_train_sample) in enumerate(x_train)\n",
    "            weights[kk] = (1/(sqrt(2*3.14)))*exp((-1/2)*((x_train_sample-x_test_sample)/bandwidth)^2)\n",
    "        end\n",
    "        y_test_gaussian[i] = (sum(weights.*y_train))/sum(weights)\n",
    "        \n",
    "        var[i] = sum(weights.^2)*sigma*sigma/(sum(weights)^2)\n",
    "    end\n",
    "    n = length(x_train)\n",
    "    \n",
    "    variance_analytical = sum(var)/n\n",
    "    \n",
    "    bias_analytical = sum((y_train .- y_test_gaussian).^2)/n\n",
    "    \n",
    "    p_true = PGFPlots.Plots.Scatter(x_train,y_train,legendentry=\"ground truth\")\n",
    "    p_analytical = PGFPlots.Plots.Scatter(x_train,y_test_gaussian,legendentry=\"analytical MSE\")\n",
    "    p_axis = PGFPlots.Axis([p_true,p_analytical],xlabel=\"x\",ylabel=\"y\")\n",
    "#     display(p_axis)\n",
    "    return bias_analytical + variance_analytical\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Run the Gaussian kernel on noise free data for the MSE calculation\n",
    "gaussian_kernel_for_MSE(0.1,1/99,1.,99,x,r_x,1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function: Gaussian kernel to compute sampling based MSE\n",
    "\"\"\"\n",
    "- Returns the square loss of the dataset\n",
    "- Is subsequently used to compute the sampling based MSE by averaging the square loss over diff datasets\n",
    "\n",
    "# Examples\n",
    "```julia\n",
    "gaussian_kernel_square_loss(0.1,1/99,1.,99,x,y,r_x)\n",
    "```\n",
    "\"\"\"\n",
    "function gaussian_kernel_square_loss(bandwidth,lo,hi,num_test,x_train,y_train,y_true)\n",
    "    x_test = make_bins(num_test,lo,hi)\n",
    "    y_test_gaussian = fill(0.,length(x_test),)\n",
    "    for (i,x_test_sample) in enumerate(x_test)\n",
    "        weights = fill(0.,length(x_train),)\n",
    "\n",
    "        for (kk,x_train_sample) in enumerate(x_train)\n",
    "            weights[kk] = (1/(sqrt(2*3.14)))*exp((-1/2)*((x_train_sample-x_test_sample)/bandwidth)^2)\n",
    "        end\n",
    "        y_test_gaussian[i] = (sum(weights.*y_train))/sum(weights)\n",
    "    end\n",
    "    return sum((y_true .- y_test_gaussian).^2)/num_test\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Perform sampling based MSE compute on 100 datasets\n",
    "n = 99\n",
    "x = fill(0.,n,)\n",
    "\n",
    "sigma = 1.\n",
    "for i in 1:n\n",
    "    x[i] = i/n\n",
    "end\n",
    "r_x = sin.(2*pi*x)\n",
    "seeds = collect(1:100)\n",
    "square_loss = fill(0.,length(seeds),)\n",
    "for seed in seeds\n",
    "    Random.seed!(seed)\n",
    "    y = r_x + rand(Normal(0,sigma),n)\n",
    "    square_loss[seed] = gaussian_kernel_square_loss(0.1,1/99,1.,99,x,y,r_x)\n",
    "end\n",
    "\n",
    "count = 0; cumsumloss=0\n",
    "expected_sq_loss = fill(0.,length(square_loss),)\n",
    "for loss in square_loss\n",
    "    cumsumloss += loss\n",
    "    count += 1\n",
    "    expected_sq_loss[count] = cumsumloss/count\n",
    "end\n",
    "print(\"Final expected_sq_loss = $(expected_sq_loss[end])\\n\")\n",
    "fig_b2 = PGFPlots.Axis([PGFPlots.Plots.Scatter(collect(1:100),expected_sq_loss)],\n",
    "    xlabel=\"Dataset number\",ylabel=\"Average sampling MSE\")\n",
    "display(fig_b2)\n",
    "PGFPlots.save(\"figs/b2.tikz\",fig_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "- $r(x) = sin(10 \\pi x),\\sigma=0.1,n = 99$\n",
    "- Plot for bandwidth = $0.001,0.05,0.2,100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function: Plot variance bands around the expected estimate using Gaussian kernel\n",
    "\"\"\"\n",
    "# Examples\n",
    "```julia\n",
    "gaussian_kernel_with_variance_bands(0.05,1/n,1,n,x,r_x,sigma)\n",
    "```\n",
    "\"\"\"\n",
    "function gaussian_kernel_with_variance_bands(bandwidth,lo,hi,num_test,x_train,y_train,sigma)\n",
    "    x_test = make_bins(num_test,lo,hi)\n",
    "    y_test_gaussian = fill(0.,length(x_test),)\n",
    "    var = fill(0.,length(x_test),)\n",
    "    for (i,x_test_sample) in enumerate(x_test)\n",
    "        weights = fill(0.,length(x_train),)\n",
    "\n",
    "        for (kk,x_train_sample) in enumerate(x_train)\n",
    "            weights[kk] = (1/(sqrt(2*3.14)))*exp((-1/2)*((x_train_sample-x_test_sample)/bandwidth)^2)\n",
    "        end\n",
    "        y_test_gaussian[i] = (sum(weights.*y_train))/sum(weights)\n",
    "        \n",
    "        var[i] = sum(weights.^2)*sigma*sigma/(sum(weights)^2)\n",
    "    end\n",
    "    \n",
    "    p_true = PGFPlots.Plots.Linear(x_train,y_train,legendentry=\"ground truth\",style=\"red\")\n",
    "    p_expected = PGFPlots.Plots.Linear(x_train,y_test_gaussian,legendentry=\"exp\", style=\"blue\")\n",
    "    p_add = PGFPlots.Plots.Linear(x_train,y_test_gaussian+sqrt.(var),legendentry=L\"exp+\\sqrt{var}\",\n",
    "    style=\"gray\")\n",
    "    p_subtract = PGFPlots.Plots.Linear(x_train,y_test_gaussian-sqrt.(var),legendentry=L\"exp-\\sqrt{var}\",\n",
    "    style = \"yellow\")\n",
    "    p_noisy = PGFPlots.Plots.Scatter(x,y_train+rand(Normal(0,sigma),num_test),legendentry=\"noisy data\")\n",
    "    p_axis = PGFPlots.Axis([p_true,p_expected,p_add,p_subtract,p_noisy],\n",
    "        xlabel=\"x\",ylabel=\"y\",title=\"bandwidth=$(bandwidth)\",legendPos=\"outer north east\")\n",
    "    return p_axis\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 99;sigma=0.1\n",
    "bandwidth = 100\n",
    "x= fill(0.,n,)\n",
    "for i in 1:n\n",
    "    x[i] = i/n\n",
    "end\n",
    "r_x = sin.(10*pi*x)\n",
    "fig_b3 = gaussian_kernel_with_variance_bands(bandwidth,1/n,1,n,x,r_x,sigma)\n",
    "display(fig_b3)\n",
    "PGFPlots.save(\"figs/b3_$(bandwidth).tikz\",fig_b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "- $r(x) = 0.5,\\sigma=1,n=99$\n",
    "- Plot for bandwidths: 0.001,0.05,0.2,100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 99;sigma=1\n",
    "bandwidth = 100\n",
    "x= fill(0.,n,)\n",
    "for i in 1:n\n",
    "    x[i] = i/n\n",
    "end\n",
    "r_x = fill(0.5,length(x),)\n",
    "fig_b4 = gaussian_kernel_with_variance_bands(bandwidth,1/n,1,n,x,r_x,sigma)\n",
    "display(fig_b4)\n",
    "PGFPlots.save(\"figs/b4_$(bandwidth).tikz\",fig_b4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "- $r(x)=cos(2\\pi x),\\sigma=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function: Analytical MSE as a function of number of data points and bandwidth\n",
    "\"\"\"\n",
    "Compute the analytical MSE using the Gaussian kernel\n",
    "\n",
    "# Examples\n",
    "```julia\n",
    "analytical_MSE(num_points,1,bandwidth)\n",
    "```\n",
    "\"\"\"\n",
    "function analytical_MSE(n,sigma,bandwidth)\n",
    "    x = fill(0.,n)\n",
    "    for i in 1:n\n",
    "        x[i]=i/n\n",
    "    end\n",
    "    r_x = cos.(2*pi*x)\n",
    "    MSE_anal = gaussian_kernel_for_MSE(bandwidth,1/n,1.,n,x,r_x,sigma)\n",
    "    return MSE_anal\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = [5, 20, 80, 320, 1280]\n",
    "bandwidths = [0.02, 0.03, 0.048, 0.063, 0.08, 0.1, 0.12, 0.15, 0.19]\n",
    "best_bandwidth = fill(0.,length(num_examples),)\n",
    "for (i,num_points) in enumerate(num_examples)\n",
    "    print(\"n = $(num_points)\\n\")\n",
    "    MSE_anal_min = 10e6\n",
    "    h_min = 100\n",
    "    for bandwidth in bandwidths\n",
    "#         print(\"h = $(bandwidth)\\n\")\n",
    "        MSE_anal = analytical_MSE(num_points,1,bandwidth)\n",
    "        if MSE_anal < MSE_anal_min\n",
    "#             print(\"MSE_anal = $(MSE_anal)\\n\")\n",
    "            MSE_anal_min = MSE_anal\n",
    "            h_min = bandwidth\n",
    "        end\n",
    "    end\n",
    "#     print(\"best_bandwidth = $(h_min)\\n\")\n",
    "    best_bandwidth[i] = h_min\n",
    "end\n",
    "@show best_bandwidth\n",
    "fig_5 = PGFPlots.Axis([PGFPlots.Plots.Scatter(num_examples,best_bandwidth)],\n",
    "    xlabel=\"Number of data points\",ylabel=\"Best bandwidth\")\n",
    "display(fig_5)\n",
    "PGFPlots.save(\"figs/b5.tikz\",fig_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **********************Homework 2 begins***********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function: Local linear regression\n",
    "\"\"\"\n",
    "    function local_linear_reg(bandwidth,lo,hi,num_test,x_train,y_train)\n",
    "\n",
    "# How it works\n",
    "- Uses the Gaussian kernel\n",
    "- Implements the formula given in the problem statement\n",
    "\n",
    "# Examples\n",
    "```julia\n",
    "local_linear_reg(0.02,lo_g,hi_g,num_test_g,x_train_g,y_train_g)\n",
    "```\n",
    "\"\"\"\n",
    "function local_linear_reg(bandwidth,lo,hi,num_test,x_train,y_train)\n",
    "    x_test = make_bins(num_test,lo,hi)\n",
    "    y_test_local_lin_reg = fill(0.,length(x_test),)\n",
    "    for (i,x_test_sample) in enumerate(x_test)\n",
    "        w = fill(0.,length(x_train),)\n",
    "        u = fill(0.,length(x_train),)\n",
    "        \n",
    "        # Compute the w and u\n",
    "        for (kk,x_train_sample) in enumerate(x_train)\n",
    "            u[kk] = x_train_sample-x_test_sample\n",
    "            w[kk] = (1/(sqrt(2*3.14)))*exp((-1/2)*(u[kk]/bandwidth)^2)\n",
    "        end\n",
    "        \n",
    "        # Compute the summation components within b\n",
    "        b_first = 0.;b_second = 0.;\n",
    "        for ii in 1:length(x_train)\n",
    "            b_first += w[ii]*u[ii]*u[ii]\n",
    "            b_second += w[ii]*u[ii]\n",
    "        end\n",
    "        \n",
    "        # Compute b\n",
    "        b = fill(0.,length(x_train),)\n",
    "        for mm in 1:length(x_train)\n",
    "            b[mm] = w[mm]*(b_first-u[mm]*b_second)\n",
    "        end\n",
    "        \n",
    "        l = fill(0.,length(x_train),)\n",
    "        for ff in 1:length(x_train)\n",
    "            l[ff] = b[ff]/sum(b)\n",
    "        end\n",
    "        \n",
    "        y_test_local_lin_reg[i] = (sum(l.*y_train))\n",
    "    end\n",
    "    plot_local_lin_reg = PGFPlots.Plots.Scatter(x_test,y_test_local_lin_reg,\n",
    "        legendentry=\"h = $(bandwidth)\");\n",
    "    return plot_local_lin_reg\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = readdlm(\"glass.dat\")\n",
    "x_train_g=Float64.(g[2:end,5])\n",
    "y_train_g = Float64.(g[2:end,2])\n",
    "plot_trainingdata = PGFPlots.Plots.Scatter(x_train_g,y_train_g,legendentry=\"Train\");\n",
    "\n",
    "lo_g=0.29\n",
    "hi_g=3.50\n",
    "num_test_g=200\n",
    "bandwidth_g=0.321;\n",
    "\n",
    "p1 = local_linear_reg(0.02,lo_g,hi_g,num_test_g,x_train_g,y_train_g)\n",
    "p2 = local_linear_reg(0.15,lo_g,hi_g,num_test_g,x_train_g,y_train_g)\n",
    "p3 = local_linear_reg(1.,lo_g,hi_g,num_test_g,x_train_g,y_train_g)\n",
    "fig = PGFPlots.Axis([plot_trainingdata,p1,p2,p3],title=\"Glass Dataset\",\n",
    "    xlabel=\"Aluminium Content\",ylabel=\"Refractive Index\",legendPos=\"outer north east\")\n",
    "display(fig)\n",
    "PGFPlots.save(\"figs/hw2/a1.tikz\",fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part a.2\n",
    "$r(x) = x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = readdlm(\"a2data.csv\",',')\n",
    "x_train = Float64.(data[2:end,2])\n",
    "y_train = Float64.(data[2:end,3])\n",
    "p_data = PGFPlots.Plots.Scatter(x_train,y_train,legendentry=\"data\")\n",
    "p_local_lin_reg = local_linear_reg(0.46,0.,1.,200,x_train,y_train)\n",
    "p_gaussian_kernel= gaussian_kernel(0.05,0.,1.,200,x_train,y_train)\n",
    "p_truth = PGFPlots.Plots.Scatter(collect(0:0.01:1),collect(0:0.01:1),legendentry=\"true function\")\n",
    "a2 = PGFPlots.Axis([p_data,p_local_lin_reg,p_gaussian_kernel,p_truth],\n",
    "    xlabel=\"x\",ylabel=\"y\",legendPos=\"outer north east\",title=\"Synthetic Data: Local linear vs Kernel estimator\")\n",
    "display(a2)\n",
    "PGFPlots.save(\"figs/hw2/a2.tikz\",a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up glass data i.e. leave only what's required for python to load in to run splines\n",
    "g = readdlm(\"glass.dat\")\n",
    "x_train_g=Float64.(g[2:end,5])\n",
    "y_train_g = Float64.(g[2:end,2])\n",
    "\n",
    "x_train = reshape(x_train_g,length(x_train_g),1)\n",
    "y_train = reshape(y_train_g,length(y_train_g),1)\n",
    "glass_cleaned = hcat(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"glass_cleaned.txt\", \"w\") do io\n",
    "    writedlm(io,glass_cleaned)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C.2\n",
    "$r(x) = cos(2\\pi x)$ and $r(x)=x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function c2(x)\n",
    "    if 0<=x && x<1\n",
    "        return cos(12*pi*x)\n",
    "    end\n",
    "    if 1 <= x && x<=2\n",
    "        return x\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = make_bins(200,0.,2.)\n",
    "y_train = c2.(x_train)\n",
    "p_true = PGFPlots.Plots.Scatter(x_train,y_train,legendentry=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_local_lin_reg = local_linear_reg(0.015,0.,2.,200,data_provided[:,1],data_provided[:,2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spline_data = reshape(readdlm(\"spline.txt\"),200,)\n",
    "p_spline= PGFPlots.Plots.Scatter(x_train,spline_data,legendentry=\"spline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_provided = readdlm(\"c2_provided_data.txt\")\n",
    "p_provided_data = PGFPlots.Plots.Scatter(data_provided[:,1],data_provided[:,2],legendentry=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_c2 = PGFPlots.Axis([p_true,p_provided_data,p_spline,p_local_lin_reg],\n",
    "    xlabel=\"x\",ylabel=\"y\",title=\"Synthetic data: Spline vs Local Linear Regression\",legendPos=\"outer north east\")\n",
    "display(p_c2)\n",
    "PGFPlots.save(\"figs/hw2/p_c2.tikz\",p_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## problem b: Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function: Local linear regression\n",
    "\"\"\"\n",
    "    function loocv_local_linear_reg(bandwidth,lo,hi,x_train,y_train)\n",
    "- Computes the leave one out cross validation score\n",
    "\n",
    "# How it works\n",
    "- Core code is same as the local linear regression\n",
    "- Implements the formula given in the problem statement\n",
    "\n",
    "# Examples\n",
    "```julia\n",
    "local_linear_reg(0.02,lo_g,hi_g,num_test_g,x_train_g,y_train_g)\n",
    "```\n",
    "\"\"\"\n",
    "function loocv_local_linear_reg(bandwidth,lo,hi,x_train,y_train)\n",
    "    n = length(x_train)\n",
    "    running_sum = 0\n",
    "    for (i,train_sample) in enumerate(x_train)\n",
    "        w = fill(0.,length(x_train),)\n",
    "        u = fill(0.,length(x_train),)\n",
    "        \n",
    "        # Compute the w and u\n",
    "        for (kk,x_train_sample) in enumerate(x_train)\n",
    "            u[kk] = x_train_sample-train_sample\n",
    "            w[kk] = (1/(sqrt(2*3.14)))*exp((-1/2)*(u[kk]/bandwidth)^2)\n",
    "        end\n",
    "        \n",
    "        # Compute the summation components within b\n",
    "        b_first = 0.;b_second = 0.;\n",
    "        for ii in 1:length(x_train)\n",
    "            b_first += w[ii]*u[ii]*u[ii]\n",
    "            b_second += w[ii]*u[ii]\n",
    "        end\n",
    "        \n",
    "        # Compute b\n",
    "        b = fill(0.,length(x_train),)\n",
    "        for mm in 1:length(x_train)\n",
    "            b[mm] = w[mm]*(b_first-u[mm]*b_second)\n",
    "        end\n",
    "        \n",
    "        l = fill(0.,length(x_train),)\n",
    "        for ff in 1:length(x_train)\n",
    "            l[ff] = b[ff]/sum(b)\n",
    "        end\n",
    "        \n",
    "        rhat_xi = (sum(l.*y_train))\n",
    "        \n",
    "        running_sum += ((y_train[i] - rhat_xi)/(1-l[i]))^2\n",
    "    end\n",
    "    return running_sum/n\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script: Variatoin of LOOCV with bandwidth\n",
    "m = readdlm(\"motor.dat\"); y_train = Float64.(m[2:end,2]); x_train = Float64.(m[2:end,1]);\n",
    "bandwidths = collect(150:1:200)*0.01\n",
    "Rhat_h = fill(0.,length(bandwidths),)\n",
    "lo=2.4\n",
    "hi=57.6\n",
    "for (i,h) in enumerate(bandwidths)\n",
    "    Rhat_h[i] = loocv_local_linear_reg(h,lo,hi,x_train,y_train)\n",
    "end\n",
    "p = PGFPlots.Axis(PGFPlots.Plots.Scatter(bandwidths,Rhat_h),\n",
    "    title=\"LOOCV vs bandwidth for motor dataset\",xlabel=\"h\",ylabel=L\"\\hat{R}_h\")\n",
    "display(p)\n",
    "PGFPlots.save(\"figs/hw2/b1.tikz\",p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal bandwidth performance\n",
    "h_opt = bandwidths[argmin(Rhat_h)]\n",
    "plot_trainingdata = PGFPlots.Plots.Scatter(x_train,y_train,legendentry=\"Data\")\n",
    "plot_local_lin_reg = local_linear_reg(h_opt,lo,hi,200,x_train,y_train)\n",
    "b2 = PGFPlots.Axis([plot_trainingdata,plot_local_lin_reg],xlabel=\"x\",ylabel=\"y\",\n",
    "    title=\"Local linear regression with optimal bandwidth for motor dataset\",legendPos=\"outer north east\")\n",
    "display(b2)\n",
    "PGFPlots.save(\"figs/hw2/b2.tikz\",b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 4: 2 layer, wide neural network **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Read in the data from provided data file\n",
    "data = readdlm(\"datahw4.csv\",',')\n",
    "x = vec(data[:,1])\n",
    "y = vec(data[:,2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# functions: indicator and relu\n",
    "function relu(x)\n",
    "    return max(0,x)\n",
    "end\n",
    "\n",
    "function indicator(x)\n",
    "    if x<0\n",
    "        return 0\n",
    "    else\n",
    "        return 1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function: the neural net\n",
    "\"\"\"\n",
    "- The neural network\n",
    "- a,b,w are vectors of length m initialized above and used globally by all functions\n",
    "\"\"\"\n",
    "function f(x,a,b,w)\n",
    "    return sum(a.*relu.(w*x+b))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     9
    ]
   },
   "outputs": [],
   "source": [
    "# function: neural net compute on a vector input rather than scalar input\n",
    "\"\"\"\n",
    "Deploy neural net on vector of inputs rather than one input x\n",
    "\n",
    "# Examples\n",
    "```julia\n",
    "f_xvector(x,a,b,w)\n",
    "```\n",
    "\"\"\"\n",
    "function f_xvector(x,a,b,w)\n",
    "    n=length(x)\n",
    "    f_vec = vec(fill(0.,n))\n",
    "    for i in 1:n\n",
    "        f_vec[i] = f(x[i],a,b,w)\n",
    "    end\n",
    "    return f_vec\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function: initialize parameters\n",
    "\"\"\"\n",
    "- Initialize the neural net parameters using a normal distribution wih `stddev`\n",
    "\n",
    "# Examples\n",
    "```julia\n",
    "a,b,w = init_params(10,1.0)\n",
    "```\n",
    "\"\"\"\n",
    "function init_params(m,stddev)\n",
    "    init_dist = Normal(0,stddev)\n",
    "    a = vec(rand(init_dist,m))\n",
    "    b = vec(rand(init_dist,m))\n",
    "    w = vec(rand(init_dist,m))\n",
    "    return a,b,w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function: jth element of gradients computation\n",
    "\"\"\"\n",
    "- Compute element `j` of gradient with respect to bias parameter `b`\n",
    "- `b` is a vector of length `m`. This function is returning the jth element of the gradient vector\n",
    "- `x` and `y` are training data of length `n`\n",
    "\n",
    "# Examples\n",
    "```julia\n",
    "# Compute each 5th element for gradient with respect to a,w and b respectively\n",
    "x = vec([1 2 3])\n",
    "y = vec([4 5 6])\n",
    "a = vec([6 3 0 8 6 2])\n",
    "b = vec([9 8 1 2 8 0])\n",
    "w = vec([5 3 0 2 1 1])\n",
    "compute_grad_j(x,y,0.1,5)\n",
    "# Expected answer: (2671.933333333333, 3404.1, 1578.0)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "function compute_grad_j(x,y,lambda,j,a,b,w)\n",
    "    n = length(x)\n",
    "    grad_a = (1/n)*sum((f_xvector(x,a,b,w)-y).*relu.(w[j]*x.+b[j])) + lambda*a[j]\n",
    "    grad_w = (1/n)*sum((f_xvector(x,a,b,w)-y).*a[j].*indicator.(w[j]*x.+b[j]).*x) + lambda*w[j]\n",
    "    grad_b = (1/n)*sum((f_xvector(x,a,b,w)-y).*a[j].*indicator.(w[j]*x.+b[j]))\n",
    "    \n",
    "    return grad_a,grad_w,grad_b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function: one step of gradient descent\n",
    "\"\"\"\n",
    "- Performs one step of gradient descent and returns updated neural net parameters\n",
    "\"\"\"\n",
    "function update_params(x,y,eta,a,b,w,lambda)\n",
    "    m = length(a)\n",
    "    gradvec_a = fill(0.,m,)\n",
    "    gradvec_w = fill(0.,m)\n",
    "    gradvec_b = fill(0.,m)\n",
    "    for j in 1:m\n",
    "        gradvec_a[j],gradvec_w[j],gradvec_b[j] = compute_grad_j(x,y,lambda,j,a,b,w)\n",
    "    end\n",
    "    return a.-eta.*gradvec_a, w.-eta.*gradvec_w, b.-eta.*gradvec_b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function: T steps of gradient descent\n",
    "\"\"\"\n",
    "- Runs `T` iterations of gradient descent and returns final parameter values `a`,`b`,`w`\n",
    "\"\"\"\n",
    "function iterate(x,y,eta,a,b,w,T,lambda)\n",
    "    for iternum in 1:T\n",
    "        print(\"iterate says: iternum = $(iternum)\")\n",
    "        a,b,w = update_params(x,y,eta,a,b,w,lambda)\n",
    "    end\n",
    "    return a,b,w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b: m = 10\n",
    "a,b,w = init_params(10,1.0)\n",
    "eta = 0.01\n",
    "lambda = 0.0\n",
    "numiter = 1e5\n",
    "at,bt,wt = iterate(x,y,eta,a,b,w,numiter,lambda)\n",
    "xtest = range(0,stop=1,length=400)\n",
    "ytest = vec(fill(0.0,length(xtest)))\n",
    "for i in 1:length(ytest)\n",
    "    ytest[i] = f(xtest[i],at,bt,wt)\n",
    "end\n",
    "p_train = PGFPlots.Plots.Scatter(x,y,legendentry = \"training data\")\n",
    "p_test = PGFPlots.Plots.Scatter(xtest,ytest,legendentry=\"test\")\n",
    "p = PGFPlots.Axis([p_train,p_test],xlabel=\"x\",ylabel=\"y\",title=\"training data and fit function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c: m = 200,stdev=2.0,eta = 0.001,numiter=1e5\n",
    "a,b,w = init_params(200,2.0)\n",
    "eta = 0.001\n",
    "lambda = 0.0\n",
    "numiter = 1e5\n",
    "at,bt,wt = iterate(x,y,eta,a,b,w,numiter,lambda)\n",
    "xtest = range(0,stop=1,length=400)\n",
    "ytest = vec(fill(0.0,length(xtest)))\n",
    "for i in 1:length(ytest)\n",
    "    ytest[i] = f(xtest[i],at,bt,wt)\n",
    "end\n",
    "p_train = PGFPlots.Plots.Scatter(x,y,legendentry = \"training data\")\n",
    "p_test = PGFPlots.Plots.Scatter(xtest,ytest,legendentry=\"test\")\n",
    "p = PGFPlots.Axis([p_train,p_test],xlabel=\"x\",ylabel=\"y\",title=\"training data and fit function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
